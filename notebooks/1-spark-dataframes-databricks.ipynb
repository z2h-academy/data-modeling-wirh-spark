{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1436a0-3357-4850-b507-a12c76e60c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Dataframes and Operations Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8b85703-a9de-4ac7-892a-b1fb92ac4442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Dataframe Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d657aa-d962-424d-a9e4-d30e96aa4c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear la sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MiNotebookConSpark\") \\\n",
    "    .getOrCreate()  # Esto automáticamente usa el modo local dentro del contenedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0269a412-f57f-4c05-b079-4f7236b5cbc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Dataframe from list of rows"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data_df = spark.createDataFrame([\n",
    "    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\n",
    "    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\n",
    "    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b00e29-29b9-47c6-9353-aecc105f5aba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Dataframe from list of rows using schema"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data_df = spark.createDataFrame([\n",
    "    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),\n",
    "    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),\n",
    "    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))\n",
    "], schema=' col_1 long, col_2 double, col_3 string, col_4 date, col_5 timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0238c11-4d56-4175-89b2-bb4ddcc4976a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Dataframe from pandas dataframe"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    'col_1': [100, 200, 400],\n",
    "    'col_2': [200., 300., 500.],\n",
    "    'col_3': ['string_test_1', 'string_test_2', 'string_test_3'],\n",
    "    'col_4': [date(2023, 1, 1), date(2023, 2, 1), date(2023, 3, 1)],\n",
    "    'col_5': [datetime(2023, 1, 1, 12, 0), datetime(2023, 1, 2, 12, 0), datetime(2023, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bff161-0a2e-4a35-93b1-0737c7acdeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-------------------+\n|col_1|col_2|        col_3|     col_4|              col_5|\n+-----+-----+-------------+----------+-------------------+\n|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|\n|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|\n|  300|400.0|string_test_3|2023-03-01|2023-01-03 12:00:00|\n+-----+-----+-------------+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Crear directamente un DataFrame sin usar sparkContext\n",
    "data = [\n",
    "    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
    "    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n",
    "    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n",
    "]\n",
    "\n",
    "# Crear DataFrame directamente desde la lista de datos\n",
    "data_df = spark.createDataFrame(data, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1791d2e-ff43-4557-ad31-a2d92c3a21a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),\n",
    "    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),\n",
    "    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))\n",
    "])\n",
    "data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d87a4498-fa76-444e-984b-25cec32fb37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to View the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e5a140-b2bd-4cf3-8798-6fecb6164064",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing DataFrames "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-------------------+\n|col_1|col_2|        col_3|     col_4|              col_5|\n+-----+-----+-------------+----------+-------------------+\n|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|\n|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|\n|  300|400.0|string_test_3|2023-03-01|2023-01-03 12:00:00|\n+-----+-----+-------------+----------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465e9144-4bf7-472d-bb62-35dad761c240",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing top n rows"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-------------------+\n|col_1|col_2|        col_3|     col_4|              col_5|\n+-----+-----+-------------+----------+-------------------+\n|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|\n|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|\n+-----+-----+-------------+----------+-------------------+\nonly showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "data_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3882bfa7-6fa4-4a7c-b039-d919edc5fb07",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing DataFrame schema"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- col_1: long (nullable = true)\n |-- col_2: double (nullable = true)\n |-- col_3: string (nullable = true)\n |-- col_4: date (nullable = true)\n |-- col_5: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b81183-36c8-4862-b2fb-4778ceb6d16c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing data vertically"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------\n col_1 | 100                 \n col_2 | 200.0               \n col_3 | string_test_1       \n col_4 | 2023-01-01          \n col_5 | 2023-01-01 12:00:00 \nonly showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "data_df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18974b22-4fae-4787-aacb-67eb458c20a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing columns of data "
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['col_1', 'col_2', 'col_3', 'col_4', 'col_5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f607973a-991a-4892-ada8-a6f8e2daf5d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Counting number of rows of data"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3513085f-5679-4735-9085-7e7b3de398b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Viewing summary statistics "
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-------------+\n|summary|col_1|col_2|        col_3|\n+-------+-----+-----+-------------+\n|  count|    3|    3|            3|\n|   mean|200.0|300.0|         NULL|\n| stddev|100.0|100.0|         NULL|\n|    min|  100|200.0|string_test_1|\n|    max|  300|400.0|string_test_3|\n+-------+-----+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.select('col_1', 'col_2', 'col_3').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "344a544c-327a-46fc-97bd-14e746a7034d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550f6a6b-61cc-4913-9220-9cb02962045c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Collecting the data"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0)),\n",
       " Row(col_1=200, col_2=300.0, col_3='string_test_2', col_4=datetime.date(2023, 2, 1), col_5=datetime.datetime(2023, 1, 2, 12, 0)),\n",
       " Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a9456e-7fc7-4b73-bdfe-b3bd45f01f68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using take"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f5c4426-d270-40be-9088-c74d727af5b1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using tail"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdd9347-da63-4575-ac6d-b55b593044ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using head"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f19f6ff2-8d56-42b8-8f21-f3a5dad2d0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Converting a PySpark DataFrame to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e48afa4-79e2-4ac5-86f0-cb95a8145ef0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Converting Pyspark dataframe to Pandas"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>200.0</td>\n",
       "      <td>string_test_1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>300.0</td>\n",
       "      <td>string_test_2</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>400.0</td>\n",
       "      <td>string_test_3</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_1  col_2          col_3       col_4               col_5\n",
       "0    100  200.0  string_test_1  2023-01-01 2023-01-01 12:00:00\n",
       "1    200  300.0  string_test_2  2023-02-01 2023-01-02 12:00:00\n",
       "2    300  400.0  string_test_3  2023-03-01 2023-01-03 12:00:00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0502651-b54a-45e6-84ae-62ea7e1600ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to do Data Manipulation - Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f34241-a0de-47d4-89d0-5596681206c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Selecting Columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|        col_3|\n+-------------+\n|string_test_1|\n|string_test_2|\n|string_test_3|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Column\n",
    "\n",
    "data_df.select(data_df.col_3).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47fd50e6-1433-4add-ae2f-1ddcfb1a0e7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating Columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-------------------+-----+\n|col_1|col_2|        col_3|     col_4|              col_5|col_6|\n+-----+-----+-------------+----------+-------------------+-----+\n|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|    A|\n|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|    A|\n|  300|400.0|string_test_3|2023-03-01|2023-01-03 12:00:00|    A|\n+-----+-----+-------------+----------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "data_df = data_df.withColumn(\"col_6\", F.lit(\"A\"))\n",
    "data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62323798-4638-483c-9d30-e9206ef826de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dropping Columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+\n|col_1|col_2|        col_3|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n|  200|300.0|string_test_2|2023-02-01|    A|\n|  300|400.0|string_test_3|2023-03-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df.drop(\"col_5\")\n",
    "data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb759316-d4e6-43b0-8ced-d073f1a20f97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Updating Columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+\n|col_1|col_2|        col_3|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|  2.0|string_test_1|2023-01-01|    A|\n|  200|  3.0|string_test_2|2023-02-01|    A|\n|  300|  4.0|string_test_3|2023-03-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.withColumn(\"col_2\", F.col(\"col_2\") / 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc409f3-6407-48a1-a5c9-21f08062a7f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Renaming Columns"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+\n|col_1|col_2|   string_col|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n|  200|300.0|string_test_2|2023-02-01|    A|\n|  300|400.0|string_test_3|2023-03-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df.withColumnRenamed(\"col_3\", \"string_col\")\n",
    "data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1755b4-eec5-44f5-b8d3-946cb0359432",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Finding Unique Values in a Column"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n|col_6|\n+-----+\n|    A|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.select(\"col_6\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beddedbc-ced1-477d-b8bd-30a102ef10dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|Total_Unique|\n+------------+\n|           1|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.select(F.countDistinct(\"col_6\").alias(\"Total_Unique\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469c586b-c14e-4652-be89-97c9b62e5818",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Change case of a Column"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+----------------+\n|col_1|col_2|   string_col|     col_4|col_6|upper_string_col|\n+-----+-----+-------------+----------+-----+----------------+\n|  100|200.0|string_test_1|2023-01-01|    A|   STRING_TEST_1|\n|  200|300.0|string_test_2|2023-02-01|    A|   STRING_TEST_2|\n|  300|400.0|string_test_3|2023-03-01|    A|   STRING_TEST_3|\n+-----+-----+-------------+----------+-----+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "data_df.withColumn('upper_string_col', upper(data_df.string_col)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b3e23f-84f5-41de-a09d-a7a20e9404b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering a Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+\n|col_1|col_2|   string_col|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.filter(data_df.col_1 == 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889da414-a8e1-4014-ab7f-5e1f10d362fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logical Operators in a Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+\n|col_1|col_2|   string_col|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.filter((data_df.col_1 == 100)\n",
    "\t\t& (data_df.col_6 == 'A')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582e12c8-1081-4a3f-a539-8fbf1080e316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+\n|col_1|col_2|   string_col|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n|  200|300.0|string_test_2|2023-02-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data_df.filter((data_df.col_1 == 100)\n",
    "\t\t| (data_df.col_2 == 300.00)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3afdd9-e883-40cc-bd79-cc9fcff57a7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Using Isin()"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+----------+-----+\n|col_1|col_2|   string_col|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n|  200|300.0|string_test_2|2023-02-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "list = [100, 200]\n",
    "data_df.filter(data_df.col_1.isin(list)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4e727a-c8b3-4010-bbde-2224b82aab79",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Datatype conversions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- col_1: integer (nullable = true)\n |-- col_2: double (nullable = true)\n |-- string_col: string (nullable = true)\n |-- col_4: string (nullable = true)\n |-- col_6: string (nullable = false)\n\n+-----+-----+-------------+----------+-----+\n|col_1|col_2|   string_col|     col_4|col_6|\n+-----+-----+-------------+----------+-----+\n|  100|200.0|string_test_1|2023-01-01|    A|\n|  200|300.0|string_test_2|2023-02-01|    A|\n|  300|400.0|string_test_3|2023-03-01|    A|\n+-----+-----+-------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\n",
    "\n",
    "data_df_2 = data_df.withColumn(\"col_4\",col(\"col_4\").cast(StringType())) \\\n",
    "    .withColumn(\"col_1\",col(\"col_1\").cast(IntegerType()))\n",
    "data_df_2.printSchema()\n",
    "data_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908430a1-d7f1-4372-aebc-ef371f1efc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- col_4: date (nullable = true)\n |-- col_1: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data_df_3 = data_df_2.selectExpr(\"cast(col_4 as date) col_4\",\n",
    "    \"cast(col_1 as long) col_1\")\n",
    "data_df_3.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08646a8d-923c-440a-bae9-305e390b529e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- col_1: double (nullable = true)\n |-- col_4: date (nullable = true)\n\n+-----+----------+\n|col_1|col_4     |\n+-----+----------+\n|100.0|2023-01-01|\n|200.0|2023-02-01|\n|300.0|2023-03-01|\n+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data_df_3.createOrReplaceTempView(\"CastExample\")\n",
    "data_df_4 = spark.sql(\"SELECT DOUBLE(col_1), DATE(col_4) from CastExample\")\n",
    "data_df_4.printSchema()\n",
    "data_df_4.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92dcefea-630b-4ecf-8649-705fbf78b93c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dropping null values from a Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Employee: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n\n+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|    John| Field-eng|  3500|\n| Michael| Field-eng|  4500|\n|  Robert|      NULL|  4000|\n|   Maria|   Finance|  3500|\n|    John|     Sales|  3000|\n|   Kelly|   Finance|  3500|\n|    Kate|   Finance|  3000|\n|  Martin|      NULL|  3500|\n|   Kiran|     Sales|  2200|\n| Michael| Field-eng|  4500|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data = [(\"John\", \"Field-eng\", 3500), \n",
    "    (\"Michael\", \"Field-eng\", 4500), \n",
    "    (\"Robert\", None, 4000), \n",
    "    (\"Maria\", \"Finance\", 3500), \n",
    "    (\"John\", \"Sales\", 3000), \n",
    "    (\"Kelly\", \"Finance\", 3500), \n",
    "    (\"Kate\", \"Finance\", 3000), \n",
    "    (\"Martin\", None, 3500), \n",
    "    (\"Kiran\", \"Sales\", 2200), \n",
    "    (\"Michael\", \"Field-eng\", 4500) \n",
    "  ]\n",
    "columns= [\"Employee\", \"Department\", \"Salary\"]\n",
    "salary_data = spark.createDataFrame(data = salary_data, schema = columns)\n",
    "salary_data.printSchema()\n",
    "salary_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7ded6f-cc9d-4bfe-8f63-afe560278772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|    John| Field-eng|  3500|\n| Michael| Field-eng|  4500|\n|   Maria|   Finance|  3500|\n|    John|     Sales|  3000|\n|   Kelly|   Finance|  3500|\n|    Kate|   Finance|  3000|\n|   Kiran|     Sales|  2200|\n| Michael| Field-eng|  4500|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50cb0f47-2786-4d8a-9514-913108e338af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dropping Duplicates from a Dataframe"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|    John| Field-eng|  3500|\n| Michael| Field-eng|  4500|\n|  Robert|      NULL|  4000|\n|    John|     Sales|  3000|\n|   Maria|   Finance|  3500|\n|   Kelly|   Finance|  3500|\n|    Kate|   Finance|  3000|\n|  Martin|      NULL|  3500|\n|   Kiran|     Sales|  2200|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "new_salary_data = salary_data.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26b09b00-3377-47f5-b54b-350d3126e92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using Aggregrates in a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151c07a1-1bf3-4cc0-98f4-46aad67e67d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Average (avg)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|avg(Salary)|\n+-----------+\n|     3520.0|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg\n",
    "salary_data.select(avg('Salary')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3104546-2fba-4638-987b-100fb9ac53cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|count(Salary)|\n+-------------+\n|           10|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.agg({'Salary':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a396838-9c08-46f9-ae42-6b5db6c094c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count distinct values"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|Distinct Salary|\n+---------------+\n|              5|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.select(countDistinct(\"Salary\").alias(\"Distinct Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d17be07-1395-470b-afec-93e6d3a8b893",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Finding maximums (max)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|max(Salary)|\n+-----------+\n|       4500|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.agg({'Salary':'max'}).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb27aef-8f62-4c89-b005-e046bc924699",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sum"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|sum(Salary)|\n+-----------+\n|      35200|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0b8f92-9d16-4cc4-9bc8-8b67db6befd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sort data with OrderBy"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n|   Kiran|     Sales|  2200|\n|    John|     Sales|  3000|\n|    Kate|   Finance|  3000|\n|    John| Field-eng|  3500|\n|   Maria|   Finance|  3500|\n|   Kelly|   Finance|  3500|\n|  Martin|      NULL|  3500|\n|  Robert|      NULL|  4000|\n| Michael| Field-eng|  4500|\n| Michael| Field-eng|  4500|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.orderBy(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021a9f0c-59f2-497e-adbe-f39a2f7b7b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n|Employee|Department|Salary|\n+--------+----------+------+\n| Michael| Field-eng|  4500|\n| Michael| Field-eng|  4500|\n|  Robert|      NULL|  4000|\n|    John| Field-eng|  3500|\n|   Maria|   Finance|  3500|\n|   Kelly|   Finance|  3500|\n|  Martin|      NULL|  3500|\n|    John|     Sales|  3000|\n|    Kate|   Finance|  3000|\n|   Kiran|     Sales|  2200|\n+--------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "salary_data.orderBy(salary_data[\"Salary\"].desc()).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 969987236417588,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1-spark-dataframes",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}